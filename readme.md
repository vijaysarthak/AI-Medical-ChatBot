# ğŸ©º Medical Chatbot (LangChain + Pinecone + OpenAI)

This project is a **Medical Question-Answering RAG Chatbot** built using LangChain, HuggingFace Embeddings, Pinecone and OpenAI GPT-4o.  
The system ingests medical PDFs, converts them into vector embeddings, stores them in Pinecone, and answers user questions using a Retrieval-Augmented Generation (RAG) pipeline.

---

## ğŸ“Œ Features

- ğŸ“„ PDF ingestion from `/data` folder  
- âœ‚ï¸ Text chunking using RecursiveCharacterTextSplitter  
- ğŸ” Semantic Search using Pinecone  
- ğŸ¤– GPT-4o as LLM backend  
- ğŸ§  Retrieval-Augmented Generation  
- ğŸ’¬ Flask-based chat UI  
- ğŸ”‘ Environment variable-based secret management  

---

## ğŸ“‚ Project Structure

```
GENAI/
â”‚
â”œâ”€â”€ data/                       # Medical PDFs
â”œâ”€â”€ public/                     # UI images
â”œâ”€â”€ research/                   # Jupyter experiments
â”‚   â””â”€â”€ trials.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ helper.py               # PDF loader, splitter, embeddings
â”‚   â”œâ”€â”€ prompt.py               # System prompt
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ static/                     # Static assets (optional)
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ chat.html               # Chat UI
â”‚
â”œâ”€â”€ app.py                      # Flask backend + RAG pipeline
â”œâ”€â”€ store_index.py              # Index creation script
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

## ğŸ§  How the RAG Pipeline Works

### 1ï¸âƒ£ Load PDFs  
Using `DirectoryLoader` to load all PDFs from `/data`.

### 2ï¸âƒ£ Filter Metadata  
Keeps only clean metadata: `{ source: file_name.pdf }`.

### 3ï¸âƒ£ Chunk Text  
Splits documents into overlapping chunks of 500 characters.

### 4ï¸âƒ£ Create Embeddings  
Uses **MiniLM-L6-v2 (384D)** from HuggingFace.

### 5ï¸âƒ£ Store/Load Pinecone Index  
Stores vector embeddings in Pinecone Serverless.

### 6ï¸âƒ£ Create Retriever  
Top-k similarity search (k=3).

### 7ï¸âƒ£ Build Chat Model  
Uses GPT-4o.

### 8ï¸âƒ£ Build RAG Chain  
Retriever + system prompt + document stuffing + GPT output.

---

## ğŸš€ Setup & Installation

###  Clone the repository

```bash
git clone <https://github.com/vijaysarthak/AI-Medical-ChatBot.git>
```


###  Install dependencies

```bash
pip install -r requirements.txt
```

###  Add your `.env` file

```
OPENAI_API_KEY=your-openai-key
PINECONE_API_KEY=your-pinecone-key
```

###  (Optional) Build vector index

```bash
python store_index.py
```



## ğŸ”§ Key Python Files

### **app.py**
- Loads embeddings  
- Loads Pinecone index  
- Creates GPT-4o RAG chain  
- Runs Flask app  
- Handles `/get` chat route  

### **src/helper.py**
Contains all helper utilities:
- `load_pdf_file()`
- `filter_to_minimal_docs()`
- `text_split()`
- `download_hugging_face_embeddings()`

### **src/prompt.py**
Defines behavior of the chatbot (system prompt).

---

## ğŸ§ª Testing the Model

Run directly in Python:

```python
response = rag_chain.invoke({"input": "What is diabetes?"})
print(response["answer"])
```

---

## ğŸ›¡ Notes / Limitations

- Not a replacement for professional medical advice  
- PDFs must contain machine-readable text  
- Pinecone index dimension must be exactly **384**  
- GPT model cost applies  

---

## ğŸš€ Future Enhancements

- Add user authentication  
- Add PDF upload from UI  
- Add streaming responses (SSE/WebSocket)  
- Deploy on Render/AWS  
- Add chat history  

---

## ğŸ‘¨â€ğŸ’» Author

**Sarthak Vijay**  
Medical Chatbot â€“ MCA Project

---